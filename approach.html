<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Approach – Baagi</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Our Approach</h1>
    <p>How we build a future of rule‑based governance</p>
  </header>
  <nav>
    <a href="index.html">Home</a>
    <a href="problem.html">Problem &amp; Vision</a>
    <a href="philosophy.html">Philosophy</a>
    <a href="approach.html">Approach</a>
    <a href="stakeholders.html">Stakeholders</a>
    <a href="services.html">Services</a>
    <a href="articles.html">Articles</a>
    <a href="contact.html">Contact</a>
  </nav>
  <div class="container">
    <section>
      <h2>1. Codify the Rules</h2>
      <p>Every fair system begins with clear rules. Many laws and procedures are already well defined but buried in legislation or manuals. We work with policymakers and lawyers to translate these rules into machine‑readable formats such as decision trees and smart contracts. Publishing these codified rules makes them transparent and gives citizens a reliable reference. It also highlights gaps or contradictions in the law that legislators can address.</p>
    </section>
    <section>
      <h2>2. Automate Routine Decisions</h2>
      <p>Once rules are codified, repeatable decisions can be handled by software. Whether approving permits, allocating benefits or auditing tax returns, algorithms can apply published criteria consistently and quickly. Used properly, automation can reduce human bias and errors. However, the UK government’s review into algorithmic decision‑making warns that data and automation can also entrench historic biases if implemented carelessly.<sup><a href="#fn1">1</a></sup> The report emphasises the need for proactive steps to identify and mitigate bias, for transparency about how algorithms work and for senior decision‑makers to retain accountability.<sup><a href="#fn1">1</a></sup> Baagi therefore advocates for explainable algorithms, regular audits and publicly available documentation.</p>
    </section>
    <section>
      <h2>3. Delegate with Oversight</h2>
      <p>Only after trust and reliability are established should certain decisions be fully delegated to automated systems. Even then, humans remain in the loop: officials oversee the system, review edge cases and handle appeals. Transparent audit trails allow independent bodies to verify that each decision followed the published rules. The same UK review stresses that transparency is key to maintaining public trust – organisational leaders must be clear that they remain accountable for decisions even when an algorithm executes them.<sup><a href="#fn1">1</a></sup> We build appeals mechanisms so that citizens can challenge automated decisions and ensure that the system continuously learns from feedback.</p>
    </section>
    <section class="footnotes">
      <h3>References</h3>
      <ol>
        <li id="fn1"><a href="https://assets.publishing.service.gov.uk/media/60142096d3bf7f70ba377b20/Review_into_bias_in_algorithmic_decision-making.pdf" target="_blank">Centre for Data Ethics and Innovation, <em>Review into bias in algorithmic decision‑making</em> (2020)</a> – the report notes that algorithms can entrench or amplify historic biases and calls for proactive steps to anticipate risks, ensure transparency and retain human accountability.</li>
      </ol>
    </section>
  </div>
  <footer>
    <p>&copy; 2025 Baagi. All rights reserved.</p>
  </footer>
</body>
</html>